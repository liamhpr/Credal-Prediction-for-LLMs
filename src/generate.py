import argparse
import os
import pathlib
import pickle
from lib2to3.pgen2.tokenize import tokenize

from src.utils import utils
import logging
import config
import datasets
import evaluate
import numpy as np
import torch
import tqdm
import wandb
from transformers import AutoModelForCausalLM, AutoTokenizer

utils.setup_logger()

parser = argparse.ArgumentParser()
parser.add_argument('--type_of_question', type=str)
parser.add_argument('--num_generations_per_prompt', type=int, default=5) # number of answers generated by the model for each prompt
parser.add_argument('--fraction_of_data_to_use', type=float, default=0.9)
parser.add_argument('--model', type=str, default='opt-6.7b')
parser.add_argument('--run_id', type=str, default='run_1')
parser.add_argument('--temperature', type=float, default=1.0) # NOTE: should be 1 for calculating the logits

#randomness of answers <1.0 the distribution becomes more peaked -> the model is more confident; >1.0 the distribution flattens -> the model is less confident and picks more random answers


parser.add_argument('--num_beams', type=int, default=1) #WARNING: changed default to 1 | number of sequences evaluated
parser.add_argument('--decoding_method', type=str, default='beam_search')
parser.add_argument('--top_p', type=float, default=1.0) # NOTE: Should be 1 for calculating the logits
parser.add_argument('--dataset', type=str, default='coqa')
args = parser.parse_args()

logging.debug('args: %s', args)

logging.info('Starting wandb')

run = wandb.init(
    # Set the wandb entity where your project will be logged (generally your team name).
    entity="liam-heppner-ludwig-maximilian-university-of-munich",
    # Set the wandb project where this run will be logged.
    project="credal-prediction-for-large-language-models",
    id=args.run_id,
    config=args,
    resume='allow'
)

wandb.init(project='TODO:', id=args.run_id, config=args, resume='allow')

run_name = wandb.run.name


# set seed value to get the same generations and results each run
seed_value = 10
os.environ['PYTHONHASHSEED'] = str(seed_value)
random.seed(seed_value)
np.random.seed(seed_value)
torch.manual_seed(seed_value)

device = 'cuda'

os.environ["HF_DATASETS_CACHE"] = config.hf_datasets_cache


model = AutoModelForCausalLM.from_pretrained(f"facebook/opt-6.7b",
                                             torch_dtype=torch.float16,
                                             cache_dir=config.hf_cache_dir).cuda()

tokenizer = AutoTokenizer.from_pretrained("facebook/opt-6.7b", use_fast=False)

if args.dataset == 'coqa':
    dataset = datasets.load_from_disk(f'{config.data_dir}/sets/coqa_dataset')
    id_to_question_mapping = dict(zip(dataset['id'], dataset['question']))
elif args.dataset == 'trivia_qa':
    raise # I did not implement the dataset yet
    #dataset = datasets.load_from_disk(f'{config.output_dir}/trivia_qa')


# Define the CoQA prompt formatting (from Kuhn's encode function)
def encode(examples):
    return tokenizer(examples['story'] + ' Q: ' + examples['question'] + ' A:', truncation=False, padding=False)

def encode_and_format_dataset(dataset):
    dataset = dataset.map(encode, batched=False, load_from_cache_files=False)
    # Ensure you keep 'id', 'answer', 'additional_answers', etc., for evaluation later
    dataset.set_format(type='torch', columns=['input_ids', 'attention_mask'], output_all_columns=True)

if args.dataset == 'coqa':
    questions = encode_and_format_dataset(train_dataset)
elif args.dataset == 'trivia_qa':
    pass
    #questions = train_dataset

dataloader = torch.utils.data.DataLoader(questions, batch_size=1)

period_token_id = tokenizer('. ')['input_ids'][1]
eos_tokens = ['Question:', ' Question:', '\n', 'Answer:', ' Answer:', 'Q:']
question_framing_ids = [[tokenizer(eos_token)['input_ids'][1]] for eos_token in eos_tokens]
squad_metric = evaluate.load("squad")
rouge = evaluate.load('rouge')
exact_match_metric = evaluate.load("exact_match")


def get_generations(model, dataloader, number_of_generations):
    """For a given model, produce a number of generation """

    with torch.no_grad():
        max_length_of_generated_sequence = 256
        sequences = []

        for batch in tqdm.tqdm(dataloader):

            input_ids = torch.cat(batch['input_ids']).to(device).reshape(
                1, -1) if args.dataset == 'trivia_qa' else batch['input_ids'].to(device)
            if args.decoding_method == 'beam_search':
                most_likely_generation = model.generate(input_ids,
                                                        num_beams=5,
                                                        num_return_sequences=2,
                                                        do_sample=False,
                                                        max_length=input_ids.shape[1] +
                                                        max_length_of_generated_sequence,
                                                        eos_token_id=period_token_id,
                                                        bad_words_ids=question_framing_ids)
            elif args.decoding_method == 'greedy':
                most_likely_generation = model.generate(input_ids,
                                                        num_beams=1,
                                                        do_sample=False,
                                                        max_length=input_ids.shape[1] +
                                                        max_length_of_generated_sequence,
                                                        eos_token_id=period_token_id,
                                                        bad_words_ids=question_framing_ids)

            input_length = input_ids.shape[1] if args.dataset == 'trivia_qa' else batch['input_ids'].shape[1]
            generations = torch.ones((number_of_generations, input_length + max_length_of_generated_sequence),
                                     dtype=torch.long,
                                     device=device)

            # NOTE: Tensor to store the total log likelihood for each sampled sequence
            log_likelihoods = torch.zeros(number_of_generations, device=device)

            for i in range(number_of_generations):

                generation_output = model.generate(input_ids,
                                            do_sample=True,
                                            num_return_sequences=1,
                                            num_beams=args.num_beams,
                                            max_length=input_ids.shape[1] + max_length_of_generated_sequence,
                                            eos_token_id=period_token_id,
                                            temperature=args.temperature,
                                            bad_words_ids=question_framing_ids,
                                            top_p=args.top_p, 
                                            output_scores=True,
                                            return_dict_in_generate=True)

                # NOTE: Extract the generated sequence IDs and the logit scores
                generation = generation_output.sequences[0] # The single generated sequence
                scores = generation_output.scores # Tuple of logits

                generations[i, :generation.shape[1]] = generation

                # NOTE: Calculate sequence Log-Likelihood
                # the generation starts after the prompt, input_ids.shape[1]

                sequence_log_prob = 0.0

                generated_ids = generation[input_ids.shape[1]:]

                assert len(scores) == len(generated_ids), "Mismatch between scores and generated tokens"
                for token_step, token_id in enumerate(generated_ids):
                    # scores[token_step] is the logits tensor for all possible next tokens at that step
                    logits = scores[token_step].squeeze(0) # Remove batch dimension
                    
                    # Convert logits to log-probabilities
                    log_probs = torch.nn.functional.log_softmax(logits, dim=-1)

                    # Get the log probability of the token that was actually chosen
                    token_log_prob = log_probs[token_id].item()
                    sequence_log_prob += token_log_prob

                # Store the total log likelihood for the current sample
                log_likelihoods[i] = sequence_log_prob


            generations = torch.reshape(generations, (-1, number_of_generations, generations.shape[-1]))
            for i in range(generations.shape[0]):

                if args.dataset == 'coqa':
                    sequence_dict = {
                        'prompt': batch['input_ids'][i].to('cpu'),
                        'generations': generations[i].to('cpu'),
                        'id': batch['id'],
                        'question': id_to_question_mapping[batch['id'][0]]
                    }
                elif args.dataset == 'trivia_qa':
                    few_shot_question = tokenizer.decode(input_ids[0])
                    question = few_shot_question.split('Question: ')[-1].split('Answer: ')[0]
                    sequence_dict = {
                        'prompt': input_ids[0],
                        'generations': generations[i],
                        'id': batch['question_id'],
                        'few_shot_question': tokenizer.decode(input_ids[0]),
                        'question': question
                    }

                generated_texts = []
                for generation in generations[i]:
                    generated_texts.append(
                        tokenizer.decode(generation[len(batch['input_ids'][i]):], skip_special_tokens=True))

                sequence_dict['generated_texts'] = generated_texts
                sequence_dict['most_likely_generation_ids'] = most_likely_generation[0].to('cpu')
                sequence_dict['most_likely_generation'] = tokenizer.decode(
                    most_likely_generation[0][len(batch['input_ids'][i]):], skip_special_tokens=True)

                sequence_dict['second_most_likely_generation_ids'] = most_likely_generation[1].to('cpu')
                sequence_dict['second_most_likely_generation'] = tokenizer.decode(
                    most_likely_generation[1][len(batch['input_ids'][i]):], skip_special_tokens=True)

                sequence_dict['semantic_variability_reference_answers'] = batch[
                    'semantic_variability'] if 'semantic_variability' in batch else None
                rouge_types = ['rouge1', 'rouge2', 'rougeL']
                for rouge_type in rouge_types:
                    if rouge_type in batch:
                        sequence_dict[rouge_type + '_reference_answers'] = batch[rouge_type]

                    else:
                        sequence_dict[rouge_type + '_reference_answers'] = None

                    sequence_dict[rouge_type + '_to_target'] = 0.0

                sequence_dict['answer'] = batch['answer']['text'] if args.dataset == 'coqa' else batch['answer']
                sequence_dict['additional_answers'] = [x[0] for x in batch['additional_answers']
                                                      ] if args.dataset == 'coqa' else None

                sequence_dict['exact_match'] = 0.0

                reference_answers = batch['answer']['text'] + [x[0] for x in batch['additional_answers']
                                                              ] if args.dataset == 'coqa' else batch['answer']

                for answer in reference_answers:
                    predictions = [sequence_dict['most_likely_generation'].lstrip()]
                    references = [answer]
                    results = exact_match_metric.compute(predictions=predictions,
                                                         references=references,
                                                         ignore_case=True,
                                                         ignore_punctuation=True)
                    sequence_dict['exact_match'] = max(results['exact_match'], sequence_dict['exact_match'])
                    rouge_results = rouge.compute(predictions=predictions, references=references)
                    for rouge_type in rouge_types:
                        sequence_dict[rouge_type + '_to_target'] = max(rouge_results[rouge_type].mid.fmeasure,
                                                                       sequence_dict[rouge_type + '_to_target'])

                sequence_dict['log_likelihoods'] = log_likelihoods.to('cpu') # Store the entire tensor of size M
                sequences.append(sequence_dict)

    return sequences


sequences = get_generations(model, dataloader, args.num_generations_per_prompt)

pathlib.Path(f'{config.output_dir}/sequences/' + run_name).mkdir(parents=True, exist_ok=True)

with open(f'{config.output_dir}/sequences/{run_name}/{args.model}_generations.pkl', 'wb') as outfile:
    pickle.dump(sequences, outfile)

